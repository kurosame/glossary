## category

ml

## titles

Support Vector Machine
SVM
サポートベクターマシン

## description

分類、回帰をサポートしており、線形、非線形にもそれぞれ対応している

### SVM 分類

特徴量が 2 つの 2 次元のデータを考えた時、SVM はその 2 つの特徴量を分割する決定境界を引く  
2 つの特徴量がそれぞれ離れた位置に属していれば、決定境界は線形分割可能  
逆に 2 つの特徴量が近い位置に存在すれば、決定境界は非線形分割することになる

決定境界線に 1 番近いインスタンスをサポートベクターと呼ぶ

- ハードマージン分類  
  全てのインスタンスが決定境界を跨がずに分類されていること  
  ハードマージン分類は外れ値の影響を受けやすい、かつ、特徴量が線形分割できる時のみ使える

- ソフトマージン分類  
  ある程度決定境界を跨ぐインスタンスを許容する分類  
  ソフトマージン分類は外れ値の影響を受けにくいため、外れ値による予測精度低下を防ぐことができる

  ソフトマージン分類の場合、マージン幅とマージン違反を許容するインスタンス数とのバランスを取る必要がある  
  この調整を C ハイパーパラメータで行う  
  C が小さければマージン幅は大きくなるが、マージン違反も増える  
  つまり、C が大きければ誤判定を許さず（ハードマージンに近づく）、C が小さければ誤判定を多く許容するようになる  
  マージン違反を規制しすぎてモデルが過学習している場合、C を小さくして正則化する必要がある

### SVM 回帰

- SVM 分類  
  マージン違反を減らしながら、マージン幅を大きく取るのを目指す  
  SVM 分類のマージン違反とは、決定境界を跨ぐインスタンスを指す

- SVM 回帰  
  マージン違反を減らしながら、マージンの中に入るインスタンスを多くするのを目指す  
  SVM 回帰のマージン違反とは、マージンの中に入っていないインスタンスを指す  
  ε ハイパーパラメータで決定境界からの誤判定のマージンを調整する  
  ε が小さければ誤差の判定が厳しく、ε が大きれば多くの誤差を正解とみなすようになる  
  ちなみに C ハイパーパラメータは誤判定となったものをどのくらい許容するかどうかのパラメータである

線形分割可能であれば、scikit-learn の LinearSVR を使えば良い

### 線形分割できないデータセットの場合

線形分割不可能なデータセットの場合は、（多項式）特徴量を追加して分割できるようにすれば良い  
scikit-learn で実装する場合、PolynomialFeatures で各特徴量の n 乗を新特徴量として追加し、StandardScaler で特徴量をスケーリングすると良い  
PolynomialFeatures の n は degree パラメータで調整する  
degree が低いと複雑なデータセットを処理できず、過小適合する可能性がある  
degree が高いと特徴量が増えすぎてパフォーマンスが落ちるし、過学習する可能性がある

### カーネルトリック

SVM はカーネルトリックをサポートしている  
カーネルトリックを使うと実際には特徴量を追加しないが、多項式特徴量を追加したかのような結果を得られる  
γ ハイパーパラメータで決定境界の複雑度を指定する  
例えば γ が小さい時は単純な直線に近づき、γ が大きい時は複雑な形となる  
モデルが過学習している時は γ を小さくし、過小適合している時は γ を大きくすると良い

### 非線形 SVM 回帰

ロト 6 予測モデルは非線形 SVM 回帰を使える SVR クラスを使って実装した

<a href="" target="_blank">サンプル</a>
